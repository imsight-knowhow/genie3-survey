# Survey on Genie 3 and Related Works

This project surveys the latest research and developments related to Google's Genie 3 model, a state-of-the-art world model capable of generating interactive environments from text prompts.

## Introduction to Genie 3

Genie 3, released by Google DeepMind in August 2025, is the first general-purpose world model capable of generating real-time interactive 3D environments from text prompts. Unlike traditional video generation models, Genie 3 creates explorable worlds that users can navigate and modify in real-time at 720p resolution and 24 frames per second.

**Key Features:**
- Real-time interactive 3D environment generation from text
- Dynamic world modification through "promptable world events"  
- Environmental consistency with up to 1 minute of visual memory
- Fusion of Genie (interactive environments) and Veo (video generation) technologies

**Applications:** Gaming, AI agent training, education, creative content, and robotics simulation.

**Related Resources:**
- **Official Blog**: [Genie 3: A New Frontier for World Models](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)
- **Demo Video**: [Genie 3: Creating dynamic worlds](https://www.youtube.com/watch?v=PDKhUknuQDg)
- **Technical Overview**: [Complete Genie 3 Interactive AI Guide](https://www.genie3.help/)

## DeepMind series

Prior works by DeepMind related to Genie 3

#### [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
- **Venue**: arXiv, 2024
- **Github Page**: [Unofficial PyTorch Implementation](https://github.com/myscience/open-genie)
- **Tutorial**: [DeepMind Genie implementation series 1 — base structure](https://dohyeongkim.medium.com/deepmind-genie-implementation-series-1-base-structure-47727338cc1a)
- **Summary**: The original paper for the Genie model, introducing a foundation world model trained on internet videos that can generate interactive 2D platformer games from a single image prompt. It is the direct predecessor to Genie 2 and 3.

#### [Genie 2: A large-scale foundation world model](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)
- **Venue**: Google DeepMind Blog, 2024
- **Summary**: Genie 2 expands on the original Genie by generating a wider variety of action-controllable, playable 3D environments. It is a key step towards the real-time, high-resolution capabilities of Genie 3.

#### [Veo 3 Tech Report](https://storage.googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf)
- **Venue**: Google DeepMind, 2024
- **Tutorial**: [Building Google Veo 3 from Scratch Using Python](https://medium.com/gitconnected/building-google-veo-3-from-scratch-using-python-80d635b08c67)
- **Summary**: Veo 3 is Google's most capable video generation model. While not a world model in the same interactive sense as Genie, its advancements in video generation, consistency, and understanding of cinematic techniques are foundational to the high-fidelity visual output of Genie 3.

#### [Scaling Instructable Agents Across Many Simulated Worlds](https://arxiv.org/abs/2404.10179)
- **Venue**: Google DeepMind, 2024
- **Github Page**: [Unofficial PyTorch Implementation](https://github.com/kyegomez/SIMA)
- **Tutorial**: [谷歌发布了可以玩3D游戏的通用AI智能体SIMA](https://zhuanlan.zhihu.com/p/687104325)
- **Summary**: The SIMA project focuses on training generalist AI agents that can follow natural language instructions across a wide range of 3D virtual environments. This is highly relevant to Genie 3, which provides the ideal training grounds for such agents.

#### [Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808)
- **Venue**: Google DeepMind, 2021
- **Summary**: This DeepMind study trains embodied agents via open-ended learning in a diverse 3D world, showing that broad task exposure leads to strong generalisation and transfer. The results highlight the value of rich, interactive environments—like those generated by Genie—for developing generally capable agents that extend beyond single-task competence.

## Neural Simulation and World Models

Directly related to Genie 3, where user can interact with a generated environment, shown as video.

#### [Back to the Features: DINO as a Foundation for Video World Models](https://arxiv.org/abs/2507.19468)
- **Venue**: arXiv, 2025
- **Summary**: DINO-world presents a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on large-scale uncurated video datasets, DINO-world learns temporal dynamics of diverse scenes from driving to simulated environments. The model demonstrates strong performance on video prediction benchmarks and physics understanding, and can be fine-tuned for action-conditioned planning. This approach significantly reduces computational requirements compared to pixel-space models while maintaining effectiveness for world modeling tasks.

#### [A Survey of Interactive Generative Video](https://arxiv.org/abs/2504.21853)
- **Venue**: arXiv, 2025
- **YouTube Demo**: [AI Video You Can Control: IGV Explained](https://www.youtube.com/watch?v=fXcBCVXVcqI)
- **Summary**: This survey provides a roadmap of real-time video generation systems for gaming, embodied AI, and driving. It focuses on the combination of generative capabilities with interactive features, which is the core of navigable world models like Genie.

#### [Toward Stable World Models: Measuring and Enhancing World Stability](https://arxiv.org/abs/2503.08122)
- **Venue**: arXiv, 2025
- **Summary**: This paper introduces the concept of "World Stability" and proposes methods to measure and enhance the content preservation capabilities of world models. This is crucial for the temporal consistency seen in Genie 3.

#### [AdaWorld: Learning Adaptable World Models with Latent Actions](https://arxiv.org/abs/2503.18938)
- **Venue**: ICML, 2025
- **Github Page**: [Little-Podi/AdaWorld](https://github.com/Little-Podi/AdaWorld)
- **Tutorial**: [VLA之外，具身+VA工作汇总](https://zhuanlan.zhihu.com/p/1929904086733557886)
- **Summary**: AdaWorld introduces a highly adaptable world model pretrained with continuous latent actions from thousands of environments. It enables zero-shot action transfer, fast adaptation, and new skill acquisition with minimal finetuning. The approach uses a latent action autoencoder to extract critical action information from videos and compress it into continuous latent actions, enabling more effective planning and controllable world model adaptation.

#### [Learning Interactive Real-World Simulators (UniSim)](https://arxiv.org/abs/2310.06114)
- **Venue**: arXiv, 2023
- **Project Page**: [universal-simulator.github.io](https://universal-simulator.github.io/)
- **Summary**: UniSim explores training a universal simulator of real-world interactions using video diffusion models. Capable of responding to both high-level instructions and low-level controls, UniSim demonstrates interactive video generation grounded in realistic physics and semantics—advancing the goal of building generally controllable world models like Genie.

#### [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
- **Venue**: arXiv, 2023
- **Tutorial**: [Multimodal LLMs for Autonomous Driving — Part 2: Transforming the Road with GAIA-1's Generative](https://medium.com/@az.tayyebi/multimodal-llms-for-autonomous-driving-part-2-transforming-the-road-with-gaia-1s-generative-b7cc06eea4e7)
- **Summary**: GAIA-1 is a generative world model specifically for autonomous driving. It can generate realistic driving scenarios from video, text, and action inputs. This work is relevant as it demonstrates the application of world models to a specific, complex, real-world domain, similar to how Genie focuses on creating interactive environments.

#### [From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](https://arxiv.org/abs/2306.12672)
- **Venue**: arXiv, 2023
- **Tutorial**: [World Models: The Blueprint for Intelligent Robotics and AGI](https://luhuihu.medium.com/world-models-215640dda513)
- **Summary**: This paper maps natural language descriptions to executable probabilistic programs that capture structured world knowledge, showing how linguistic prompts can be translated into compositional world models. The approach offers insights into bridging text-based user inputs and internal generative models, a key capability for systems like Genie that create interactive environments from textual instructions.

#### [Playable Environments: Video Manipulation in Space and Time](https://arxiv.org/abs/2203.01914)
- **Venue**: CVPR, 2022
- **Github Page**: [willi-menapace/playable-environments](https://github.com/willi-menapace/playable-environments)
- **Summary**: Playable Environments introduces a framework for interactive video generation and manipulation that allows user control over scene appearance and dynamics in both space and time. This advances real-time controllability in generative video models, directly supporting the creation of interactive worlds as envisioned by Genie.

#### [Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models](https://arxiv.org/abs/2205.13817)
- **Venue**: NeurIPS, 2022
- **Github Page**: [panmt/Iso-Dream](https://github.com/panmt/Iso-Dream)
- **Summary**: Iso-Dream extends world-model-based reinforcement learning by disentangling controllable and non-controllable visual dynamics. By isolating elements of the scene that the agent cannot influence, Iso-Dream improves policy learning and prediction accuracy. This work highlights techniques for handling complex dynamics in interactive environments, aligning with Genie's goal of robust world modeling.

#### [Mastering Atari with Discrete World Models](https://arxiv.org/abs/2010.02193)
- **Venue**: ICLR, 2021
- **Github Page**: [DreamerV2](https://github.com/danijar/dreamerv2)
- **Tutorial**: [DreamerV2 Explained](https://gordicaleksa.medium.com/how-to-get-started-with-reinforcement-learning-rl-4922fafeaf8c)
- **YouTube Demo**: [Dreamer v2 Explained](https://www.youtube.com/watch?v=o75ybZ-6Uu8)
- **Summary**: This paper introduces DreamerV2, a reinforcement learning agent that learns a world model in a discrete latent space. It was the first agent to achieve human-level performance on the Atari benchmark by learning from pixels. This work is highly relevant to Genie 3 as it demonstrates the power of learning a world model for complex, interactive tasks.

#### [Playable Video Generation](https://arxiv.org/abs/2101.12195)
- **Venue**: CVPR, 2021
- **Github Page**: [willi-menapace/PlayableVideoGeneration](https://github.com/willi-menapace/PlayableVideoGeneration)
- **Summary**: This paper introduces Playable Video Generation (PVG), an unsupervised framework that allows users to control the evolution of generated videos. By modelling video dynamics with user inputs, PVG demonstrates controllable, interactive generation of video content, an important step toward creating fully interactive environments as envisioned by Genie.

#### [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060)
- **Venue**: CVPR, 2021
- **Github Page**: [nv-tlabs/DriveGAN_code](https://github.com/nv-tlabs/DriveGAN_code)
- **Summary**: DriveGAN is a neural simulator that learns from video sequences and agent actions to create a controllable simulation. It achieves controllability by disentangling different components of the environment, such as the background, agent, and other actors. This is highly relevant to Genie's goal of creating interactive and controllable worlds.

#### [NÜWA: Visual Synthesis Pre-Training for Neural Visual World Creation](https://arxiv.org/abs/2111.12417)
- **Venue**: arXiv (to appear in ECCV 2022), 2021
- **Github Page**: [microsoft/NUWA](https://github.com/microsoft/NUWA)
- **YouTube Demo**: [Model Demo](https://www.youtube.com/watch?v=InhMx1h0N40)
- **Summary**: NÜWA is a unified multimodal generative model capable of synthesizing and editing both images and videos from diverse inputs, including text prompts. By pre-training on large-scale visual data and employing a 3D transformer architecture, NÜWA demonstrates strong generalization across visual synthesis tasks, informing the large-scale vision foundation underlying world models like Genie.

#### [Neural Game Engine: Accurate learning of generalizable forward models from pixels](https://arxiv.org/abs/2003.10520)
- **Venue**: IEEE Transactions on Games, 2020
- **Github Page**: [Neural-Game-Engine](https://github.com/Bam4d/Neural-Game-Engine)
- **Summary**: This paper introduces the Neural Game Engine (NGE), a novel architecture that learns game dynamics directly from pixel data. It can learn grid-based game environments and generalize to different level sizes, which is highly relevant for creating diverse and scalable interactive worlds like those in Genie 3.

#### [Learning to Simulate Dynamic Environments with GameGAN](https://arxiv.org/abs/2005.12126)
- **Venue**: CVPR, 2020
- **Github Page**: [nv-tlabs/GameGAN_code](https://github.com/nv-tlabs/GameGAN_code)
- **Tutorial**: [How to steal a video game using GAMEGAN](https://medium.com/@simon.moisselin/how-to-steal-a-video-game-using-gamegan-6ea77616e79f)
- **Summary**: GameGAN is a generative model that learns to visually imitate a game by observing an agent's interactions. It uses a generative adversarial network to learn the game's rules and dynamics from gameplay footage. This is an early example of learning a world model for a game environment, making it a foundational work for later models like Genie.