# References - Genie: Generative Interactive Environments

This document contains all references from the paper "Genie: Generative Interactive Environments" formatted in IEEE style.

## References

[1] I. Babuschkin et al., "The deepmind jax ecosystem," 2020. [Online]. Available: http://github.com/deepmind

[2] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, "Frozen in time: A joint video and image encoder for end-to-end retrieval," in *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021, pp. 1708–1718.

[3] B. Baker et al., "Video pretraining (vpt): Learning to act by watching unlabeled online videos," *Advances in Neural Information Processing Systems*, vol. 35, pp. 24639–24654, 2022.

[4] C. Bamford and S. M. Lucas, "Neural game engine: Accurate learning of generalizable forward models from pixels," in *Conference on Games*, 2020.

[5] J. Bauer et al., "Human-timescale adaptation in an open-ended task space," in *Proceedings of the 40th International Conference on Machine Learning*, vol. 202, 2023, pp. 1887–1935.

[6] A. Blattmann et al., "Stable video diffusion: Scaling latent video diffusion models to large datasets," 2023.

[7] A. Blattmann et al., "Align your latents: High-resolution video synthesis with latent diffusion models," *2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 22563–22575, 2023.

[8] A. Brohan et al., "Rt-1: Robotics transformer for real-world control at scale," in *Robotics: Science and Systems*, 2023.

[9] T. Brooks et al., "Video generation models as world simulators," 2024. [Online]. Available: https://openai.com/research/video-generation-models-as-world-simulators

[10] T. Brown et al., "Language models are few-shot learners," *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.

[11] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, "Maskgit: Masked generative image transformer," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 11315–11325.

[12] S. Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed, "Recurrent environment simulators," in *International Conference on Learning Representations*, 2017.

[13] A. Clark, J. Donahue, and K. Simonyan, "Efficient video generation on complex datasets," *CoRR*, abs/1907.06571, 2019.

[14] J. Clune, "Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence," *arXiv preprint arXiv:1905.10985*, 2019.

[15] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, "Leveraging procedural generation to benchmark reinforcement learning," in *Proceedings of the 37th International Conference on Machine Learning*, 2020, pp. 2048–2056.

[16] M. Dehghani et al., "Scaling vision transformers to 22 billion parameters," in *Proceedings of the 40th International Conference on Machine Learning*, vol. 202, 2023, pp. 7480–7512.

[17] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," in *International Conference on Learning Representations*, 2021.

[18] A. Edwards, H. Sahni, Y. Schroecker, and C. Isbell, "Imitating latent policies from observation," in *International conference on machine learning*, 2019, pp. 1755–1763.

[19] S. M. A. Eslami et al., "Neural scene representation and rendering," *Science*, vol. 360, no. 6394, pp. 1204–1210, 2018.

[20] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, "Structure and content-guided video synthesis with diffusion models," in *2023 IEEE/CVF International Conference on Computer Vision (ICCV)*, 2023.

[21] C. Finn, I. Goodfellow, and S. Levine, "Unsupervised learning for physical interaction through video prediction," in *Proceedings of the 30th International Conference on Neural Information Processing Systems*, NIPS'16, 2016, pp. 64–72.

[22] A. Gupta, S. Tian, Y. Zhang, J. Wu, R. Martín-Martín, and L. Fei-Fei, "Maskvit: Masked visual pre-training for video prediction," in *The Eleventh International Conference on Learning Representations*, 2023.

[23] D. Ha and J. Schmidhuber, "Recurrent world models facilitate policy evolution," in *Proceedings of the 32Nd International Conference on Neural Information Processing Systems*, NeurIPS'18, 2018, pp. 2455–2467.

[24] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to control: Learning behaviors by latent imagination," in *International Conference on Learning Representations*, 2020.

[25] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, "Mastering atari with discrete world models," in *International Conference on Learning Representations*, 2021.

[26] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016, pp. 770–778.

[27] A. Henry, P. R. Dachapally, S. S. Pawar, and Y. Chen, "Query-key normalization for transformers," in *Findings of the Association for Computational Linguistics: EMNLP 2020*, 2020, pp. 4246–4253.

[28] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, "Imagen video: High definition video generation with diffusion models," 2022.

[29] J. Ho et al., "Video diffusion models," in *Advances in Neural Information Processing Systems*, vol. 35, 2022, pp. 8633–8646.

[30] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, "Cogvideo: Large-scale pretraining for text-to-video generation via transformers," *arXiv preprint arXiv:2205.15868*, 2022.

[31] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, "Cogvideo: Large-scale pretraining for text-to-video generation via transformers," in *The Eleventh International Conference on Learning Representations*, 2023.

[32] T. Höppe, A. Mehrjou, S. Bauer, D. Nielsen, and A. Dittadi, "Diffusion models for video prediction and infilling," *Transactions on Machine Learning Research*, 2022.

[33] A. Hu et al., "Gaia-1: A generative world model for autonomous driving," 2023.

[34] J. Huang, Y. Jin, K. M. Yi, and L. Sigal, "Layered controllable video generation," in *Computer Vision – ECCV 2022: 17th European Conference*, 2022, pp. 546–564.

[35] N. P. Jouppi et al., "A domain-specific supercomputer for training deep neural networks," *Communications of the ACM*, vol. 63, no. 7, pp. 67–78, 2020.

[36] D. Kalashnikov et al., "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation," *arXiv preprint arXiv:1806.10293*, 2018.

[37] N. Kalchbrenner et al., "Video pixel networks," in *Proceedings of the 34th International Conference on Machine Learning*, vol. 70, 2017, pp. 1771–1779.

[38] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney, "Recurrent experience replay in distributed reinforcement learning," in *International conference on learning representations*, 2018.

[39] S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler, "Learning to simulate dynamic environments with gamegan," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.

[40] S. W. Kim, J. Philion, A. Torralba, and S. Fidler, "Drivegan: Towards a controllable high-quality neural simulation," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021, pp. 5820–5829.

[41] G. Le Moing, J. Ponce, and C. Schmid, "Ccvs: Context-aware controllable video synthesis," in *Advances in Neural Information Processing Systems*, vol. 34, 2021, pp. 14042–14055.

[42] W. Lotter, G. Kreiman, and D. Cox, "Deep predictive coding networks for video prediction and unsupervised learning," in *International Conference on Learning Representations*, 2017.

[43] P. Luc et al., "Transformation-based adversarial video prediction on large-scale data," *CoRR*, abs/2003.04035, 2020.

[44] W. Menapace, S. Lathuilière, S. Tulyakov, A. Siarohin, and E. Ricci, "Playable video generation," in *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021*, 2021, pp. 10061–10070.

[45] W. Menapace et al., "Playable environments: Video manipulation in space and time," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022.

[46] V. Micheli, E. Alonso, and F. Fleuret, "Transformers are sample-efficient world models," in *The Eleventh International Conference on Learning Representations*, 2023.

[47] M. S. Nunes, A. Dehban, P. Moreno, and J. Santos-Victor, "Action-conditioned benchmarking of robotic video prediction models: a comparative study," in *2020 IEEE International Conference on Robotics and Automation (ICRA)*, 2020, pp. 8316–8322.

[48] J. Oh, X. Guo, H. Lee, R. Lewis, and S. Singh, "Action-conditional video prediction using deep networks in atari games," in *Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2*, NIPS'15, 2015, pp. 2863–2871.

[49] Open Ended Learning Team et al., "Open-ended learning leads to generally capable agents," *CoRR*, abs/2107.12808, 2021.

[50] M. Oquab et al., "Dinov2: Learning robust visual features without supervision," *arXiv preprint arXiv:2304.07193*, 2023.

[51] M. Pan, X. Zhu, Y. Wang, and X. Yang, "Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models," in *Advances in Neural Information Processing Systems*, vol. 35, 2022, pp. 23178–23191.

[52] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, "Improving language understanding by generative pre-training," 2018.

[53] A. Radford et al., "Language models are unsupervised multitask learners," *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.

[54] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, "Zero: Memory optimizations toward training trillion parameter models," in *SC20: International Conference for High Performance Computing, Networking, Storage and Analysis*, 2020, pp. 1–16.

[55] A. Ramesh et al., "Zero-shot text-to-image generation," in *Proceedings of the 38th International Conference on Machine Learning*, vol. 139, 2021, pp. 8821–8831.

[56] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, "Hierarchical text-conditional image generation with clip latents," 2022.

[57] S. Reed et al., "A generalist agent," *Transactions on Machine Learning Research*, 2022.

[58] S. Risi and J. Togelius, "Increasing generality in machine learning through procedural content generation," *Nature Machine Intelligence*, vol. 2, 2020.

[59] S. Risi and J. Togelius, "Procedural content generation: From automatically generating game levels to increasing generality in machine learning," *Nature*, 2020.

[60] J. Robine, M. Höftmann, T. Uelwer, and S. Harmeling, "Transformer-based world models are happy with 100k interactions," in *The Eleventh International Conference on Learning Representations*, 2023.

[61] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 10684–10695.

[62] O. Rybkin et al., "Learning what you can do before doing anything," in *International Conference on Learning Representations*, 2019.

[63] C. Saharia et al., "Photorealistic text-to-image diffusion models with deep language understanding," in *Advances in Neural Information Processing Systems*, 2022.

[64] D. Schmidt and M. Jiang, "Learning to act without actions," in *The Twelfth International Conference on Learning Representations*, 2024.

[65] M. Shoeybi et al., "Megatron-lm: Training multi-billion parameter language models using model parallelism," *CoRR*, abs/1909.08053, 2019.

[66] U. Singer et al., "Make-a-video: Text-to-video generation without text-video data," in *The Eleventh International Conference on Learning Representations*, 2023.

[67] S. Sudhakaran et al., "Prompt-guided level generation," in *Proceedings of the Companion Conference on Genetic and Evolutionary Computation*, 2023, pp. 179–182.

[68] A. Summerville et al., "Procedural content generation via machine learning (PCGML)," *IEEE Trans. Games*, vol. 10, no. 3, pp. 257–270, 2018.

[69] G. Todd, S. Earle, M. U. Nasir, M. C. Green, and J. Togelius, "Level generation through large language models," in *Proceedings of the 18th International Conference on the Foundations of Digital Games*, 2023, pp. 1–8.

[70] F. Torabi, G. Warnell, and P. Stone, "Behavioral cloning from observation," *arXiv preprint arXiv:1805.01954*, 2018.

[71] T. Unterthiner et al., "FVD: A new metric for video generation," 2019.

[72] A. van den Oord et al., "Imagen 2," [Online]. Available: https://deepmind.google/technologies/imagen-2/

[73] A. van den Oord et al., "Neural discrete representation learning," in *Proceedings of the 31st International Conference on Neural Information Processing Systems*, NIPS'17, 2017, pp. 6309–6318.

[74] A. Vaswani et al., "Attention is all you need," in *Advances in Neural Information Processing Systems*, 2017, pp. 5998–6008.

[75] R. Villegas et al., "Phenaki: Variable length video generation from open domain textual descriptions," in *International Conference on Learning Representations*, 2023.

[76] J. C. Walker, A. Razavi, and A. van den Oord, "Predicting video with VQVAE," 2021.

[77] Y. Wang et al., "Internvid: A large-scale video-text dataset for multimodal understanding and generation," 2023.

[78] L. Wong et al., "From word models to world models: Translating from natural language to the probabilistic language of thought," 2023.

[79] C. Wu et al., "Nüwa: Visual synthesis pre-training for neural visual world creation," in *European conference on computer vision*, 2022, pp. 720–736.

[80] M. Xu, W. Dai, C. Liu, X. Gao, W. Lin, G.-J. Qi, and H. Xiong, "Spatial-temporal transformer networks for traffic flow forecasting," *arXiv preprint arXiv:2001.02908*, 2020.

[81] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, "Videogpt: Video generation using vq-vae and transformers," 2021.

[82] W. Yan, D. Hafner, S. James, and P. Abbeel, "Temporally consistent transformers for video generation," in *Proceedings of the 40th International Conference on Machine Learning*, vol. 202, 2023, pp. 39062–39098.

[83] M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, "Learning interactive real-world simulators," *arXiv preprint arXiv:2310.06114*, 2023.

[84] W. Ye, Y. Zhang, P. Abbeel, and Y. Gao, "Become a proficient player with limited data through watching pure videos," in *The Eleventh International Conference on Learning Representations*, 2022.

[85] L. Yu et al., "Magvit: Masked generative video transformer," in *2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023, pp. 10459–10469.

---

**Total References:** 85

This reference list was extracted from the paper "Genie: Generative Interactive Environments" and formatted according to IEEE citation standards.
